{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading model...\n[INFO] starting video stream...\n"
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "confThreshold = 0.5  #Confidence threshold\n",
    "\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "\n",
    "inpWidth = 416       #Width of network's input image\n",
    "\n",
    "inpHeight = 416      #Height of network's input image# import the necessary packages\n",
    "from imutils.video import VideoStream, FileVideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2 \n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "classesFile = path+\"modanet.names\"\n",
    "CLASSES = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    CLASSES = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    " \n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "#net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "#vs = VideoStream(src=0).start()\n",
    "cap = cv2.VideoCapture(\"Downloads\\\\Til_vidtest.mp4\")\n",
    "#vs = FileVideoStream(\"Til_vidtest.mp4\")\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)#added to get it to show\n",
    "# loop over the frames from the video stream\n",
    "while not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(vid)\n",
    "        print(\"loading video...\")\n",
    "\n",
    "        # esc to close stream\n",
    "        if cv2.waitKey(16) == 27:\n",
    "            break\n",
    "\n",
    "    while True:\n",
    "        flag, frame = cap.read()\n",
    "\t# grab the frame from the threaded video stream and resize it\n",
    "\t# to have a maximum width of 400 pixels\n",
    "\tflag, frame = vs.read()\n",
    "    \n",
    "\tframe = imutils.resize(frame, width=400)\n",
    "\t# grab the frame dimensions and convert it to a blob\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "\t\t0.007843, (300, 300), 127.5)\n",
    "\t# pass the blob through the network and obtain the detections and\n",
    "\t# predictions\n",
    "\tnet.setInput(blob)\n",
    "\tdetections = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "\tfor i in np.arange(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\t\t# filter out weak detections by ensuring the `confidence` is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > .20:\n",
    "\t\t\t# extract the index of the class label from the\n",
    "\t\t\t# `detections`, then compute the (x, y)-coordinates of\n",
    "\t\t\t# the bounding box for the object\n",
    "\t\t\tidx = int(detections[0, 0, i, 1])\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t\t# draw the prediction on the frame\n",
    "\t\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "\t\t\t\tconfidence * 100)\n",
    "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\tCOLORS[idx], 2)\n",
    "\t\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\t\tcv2.putText(frame, label, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "\t# show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\t# update the FPS counter\n",
    "\tfps.update()\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bag', 'belt', 'boots', 'footwear', 'outer', 'dress', 'sunglasses', 'pants', 'top', 'shorts', 'skirt', 'headwear', 'scarf/tie']\n"
    }
   ],
   "source": [
    "# # Load names of classes\n",
    "# path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "# classesFile = path+\"modanet.names\"\n",
    "# classes = None\n",
    "# with open(classesFile, 'rt') as f:\n",
    "#     classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    " \n",
    "# # Give the configuration and weight files for the model and load the network using them.\n",
    "# modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "# modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "# net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "# net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
    "# net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading model...\n[INFO] starting video stream...\n416 416\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 4 were indexed",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-64e7bcb6fa97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# extract the confidence (i.e., probability) associated with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;31m# filter out weak detections by ensuring the `confidence` is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# greater than the minimum confidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from imutils.video import VideoStream, FileVideoStream\n",
    "#from imutils.video import FPS\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2 \n",
    "\n",
    "# Initialize the parameters\n",
    "confThreshold = 0.5  #Confidence threshold\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "inpWidth = 416       #Width of network's input image\n",
    "inpHeight = 416      #Height of network's input image\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "classesFile = path+\"modanet.names\"\n",
    "CLASSES = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    CLASSES = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    " \n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "#net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "#vs = VideoStream(src=0).start()\n",
    "vs = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "#vs = FileVideoStream(\"TIL_vidtest.mp4\")\n",
    "#time.sleep(2.0)\n",
    "#fps = FPS().start()\n",
    "#cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)#added to get it to show\n",
    "#cv2.waitKey(10)\n",
    "#print(1)\n",
    "# loop over the frames from the video stream\n",
    "while not vs.isOpened():\n",
    "    vs = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "    print(\"loading video...\")\n",
    "\n",
    "    # esc to close stream\n",
    "    if cv2.waitKey(16) == 27:\n",
    "        break\n",
    "\n",
    "while vs.isOpened():\n",
    "    # grab the frame from the threaded video stream and resize it\n",
    "    # to have a maximum width of 400 pixels\n",
    "    flag, frame = vs.read()\n",
    "    #print(\"1\")\n",
    "    #frame = imutils.resize(frame, width=400)\n",
    "    #frame = cv2.resize(frame, (416, 416))\n",
    "    #print(frame)\n",
    "    #grab the frame dimensions and convert it to a blob\n",
    "    # (h, w) = frame.shape[:2]\n",
    "    h = 416 \n",
    "    w = 416\n",
    "    print(h, w)\n",
    "    blob = cv2.dnn.blobFromImage(frame,\n",
    "        1/255, (416, 416)) #mean removed with val of 127.5\n",
    "    # pass the blob through the network and obtain the detections and\n",
    "    # predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\t\n",
    "    # print(detections)\n",
    "    # print(detections.shape)\n",
    "    #loop over the detections\n",
    "    for i in np.arange(0, detections.shape[1]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > .20:\n",
    "            # extract the index of the class label from the\n",
    "            # `detections`, then compute the (x, y)-coordinates of\n",
    "            # the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "                confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "    #show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.waitKey(10)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    # update the FPS counter\n",
    "    #fps.update()\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "#fps.stop()\n",
    "#print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "#print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(10)\n",
    "#vs.stop()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading model...\n[INFO] starting video stream...\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bc70177b20e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m# Remove the bounding boxes with low confidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;31m# Put efficiency information. The function getPerfProfile returns the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bc70177b20e0>\u001b[0m in \u001b[0;36mpostprocess\u001b[1;34m(frame, outs)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdetection\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mclassId\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassId\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mconfThreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \"\"\"\n\u001b[1;32m-> 1188\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from imutils.video import VideoStream, FileVideoStream\n",
    "#from imutils.video import FPS\n",
    "import numpy as np\n",
    "\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2 \n",
    "\n",
    "# Initialize the parameters\n",
    "confThreshold = 0.5  #Confidence threshold\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "inpWidth = 416       #Width of network's input image\n",
    "inpHeight = 416      #Height of network's input image\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "classesFile = path+\"modanet.names\"\n",
    "CLASSES = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    CLASSES = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "outputFile = 'TIL_vidtest_yolo_out_py.avi'\n",
    "\n",
    "\n",
    "# Get the video writer initialized to save the output video\n",
    "\n",
    "\n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "#net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "#cap = VideoStream(src=0).start()\n",
    "cap = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "\n",
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "# Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                width = int(detection[2] * frameWidth)\n",
    "                height = int(detection[3] * frameHeight)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        drawPred(classIds[i], confidences[i], left, top, left + width, top + height)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "vid_writer = cv2.VideoWriter(outputFile, cv2.VideoWriter_fourcc('M','J','P','G'), 30, (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "\n",
    "while not cap.isOpened():\n",
    "    cap = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "    \n",
    "    print(\"loading video...\")\n",
    "\n",
    "    # esc to close stream\n",
    "    if cv2.waitKey(16) == 27:\n",
    "        break\n",
    "\n",
    "while cv2.waitKey(1) < 0:\n",
    "\n",
    "    # get frame from the video\n",
    "    hasFrame, frame = cap.read()\n",
    "    \n",
    "    # Stop the program if reached end of video\n",
    "    if not hasFrame:\n",
    "        print(\"Done processing !!!\")\n",
    "        print(\"Output file is stored as \", outputFile)\n",
    "        cv2.waitKey(3000)\n",
    "        break\n",
    "\n",
    "    # Create a 4D blob from a frame.\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n",
    "    print(\"blob created\")\n",
    "    # Sets the input to the network\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Runs the forward pass to get output of the output layers\n",
    "    outs = net.forward(getOutputsNames(net))\n",
    "    print(\"reaching post process\")\n",
    "    # Remove the bounding boxes with low confidence\n",
    "    postprocess(frame, outs)\n",
    "\n",
    "    # Put efficiency information. The function getPerfProfile returns the\n",
    "    # overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "    cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "    # Write the frame with the detection boxes\n",
    " \n",
    "    vid_writer.write(frame.astype(np.uint8))\n",
    "\n",
    "\n",
    "#     #show the output frame\n",
    "#     cv2.imshow(\"Frame\", frame)\n",
    "#     cv2.waitKey(10)\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "#     # if the `q` key was pressed, break from the loop\n",
    "#     if key == ord(\"q\"):\n",
    "#         break\n",
    "#     # update the FPS counter\n",
    "#     #fps.update()\n",
    "\n",
    "#     # stop the timer and display FPS information\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #fps.stop()\n",
    "# #print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "# #print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# # do a bit of cleanup\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(10)\n",
    "# #cap.stop()\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('TIL_vidtest.mp4')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    cv.imshow('frame', frame)\n",
    "    if cv.waitKey(10) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bittil2020pipenv2f21c983fd1542a19b317973bf347e7d",
   "display_name": "Python 3.6.8 64-bit ('TIL-2020': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}