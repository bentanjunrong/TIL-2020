{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading model...\n[INFO] starting video stream...\n"
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "confThreshold = 0.5  #Confidence threshold\n",
    "\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "\n",
    "inpWidth = 416       #Width of network's input image\n",
    "\n",
    "inpHeight = 416      #Height of network's input image# import the necessary packages\n",
    "from imutils.video import VideoStream, FileVideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2 \n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "classesFile = path+\"modanet.names\"\n",
    "CLASSES = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    CLASSES = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    " \n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "#net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "#vs = VideoStream(src=0).start()\n",
    "cap = cv2.VideoCapture(\"Downloads\\\\Til_vidtest.mp4\")\n",
    "#vs = FileVideoStream(\"Til_vidtest.mp4\")\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)#added to get it to show\n",
    "# loop over the frames from the video stream\n",
    "while not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(vid)\n",
    "        print(\"loading video...\")\n",
    "\n",
    "        # esc to close stream\n",
    "        if cv2.waitKey(16) == 27:\n",
    "            break\n",
    "\n",
    "    while True:\n",
    "        flag, frame = cap.read()\n",
    "\t# grab the frame from the threaded video stream and resize it\n",
    "\t# to have a maximum width of 400 pixels\n",
    "\tflag, frame = vs.read()\n",
    "    \n",
    "\tframe = imutils.resize(frame, width=400)\n",
    "\t# grab the frame dimensions and convert it to a blob\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "\t\t0.007843, (300, 300), 127.5)\n",
    "\t# pass the blob through the network and obtain the detections and\n",
    "\t# predictions\n",
    "\tnet.setInput(blob)\n",
    "\tdetections = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "\tfor i in np.arange(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\t\t# filter out weak detections by ensuring the `confidence` is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > .20:\n",
    "\t\t\t# extract the index of the class label from the\n",
    "\t\t\t# `detections`, then compute the (x, y)-coordinates of\n",
    "\t\t\t# the bounding box for the object\n",
    "\t\t\tidx = int(detections[0, 0, i, 1])\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t\t# draw the prediction on the frame\n",
    "\t\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "\t\t\t\tconfidence * 100)\n",
    "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\tCOLORS[idx], 2)\n",
    "\t\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\t\tcv2.putText(frame, label, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "\t# show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\t# update the FPS counter\n",
    "\tfps.update()\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bag', 'belt', 'boots', 'footwear', 'outer', 'dress', 'sunglasses', 'pants', 'top', 'shorts', 'skirt', 'headwear', 'scarf/tie']\n"
    }
   ],
   "source": [
    "# # Load names of classes\n",
    "# path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "# classesFile = path+\"modanet.names\"\n",
    "# classes = None\n",
    "# with open(classesFile, 'rt') as f:\n",
    "#     classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    " \n",
    "# # Give the configuration and weight files for the model and load the network using them.\n",
    "# modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "# modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "# net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "# net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
    "# net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading model...\n[INFO] starting video stream...\n416 416\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 4 were indexed",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-64e7bcb6fa97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# extract the confidence (i.e., probability) associated with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m# the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;31m# filter out weak detections by ensuring the `confidence` is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# greater than the minimum confidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from imutils.video import VideoStream, FileVideoStream\n",
    "#from imutils.video import FPS\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2 \n",
    "\n",
    "# Initialize the parameters\n",
    "confThreshold = 0.5  #Confidence threshold\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "inpWidth = 416       #Width of network's input image\n",
    "inpHeight = 416      #Height of network's input image\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "classesFile = path+\"modanet.names\"\n",
    "CLASSES = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    CLASSES = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    " \n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "#net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "#vs = VideoStream(src=0).start()\n",
    "vs = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "#vs = FileVideoStream(\"TIL_vidtest.mp4\")\n",
    "#time.sleep(2.0)\n",
    "#fps = FPS().start()\n",
    "#cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)#added to get it to show\n",
    "#cv2.waitKey(10)\n",
    "#print(1)\n",
    "# loop over the frames from the video stream\n",
    "while not vs.isOpened():\n",
    "    vs = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "    print(\"loading video...\")\n",
    "\n",
    "    # esc to close stream\n",
    "    if cv2.waitKey(16) == 27:\n",
    "        break\n",
    "\n",
    "while vs.isOpened():\n",
    "    # grab the frame from the threaded video stream and resize it\n",
    "    # to have a maximum width of 400 pixels\n",
    "    flag, frame = vs.read()\n",
    "    #print(\"1\")\n",
    "    #frame = imutils.resize(frame, width=400)\n",
    "    #frame = cv2.resize(frame, (416, 416))\n",
    "    #print(frame)\n",
    "    #grab the frame dimensions and convert it to a blob\n",
    "    # (h, w) = frame.shape[:2]\n",
    "    h = 416 \n",
    "    w = 416\n",
    "    print(h, w)\n",
    "    blob = cv2.dnn.blobFromImage(frame,\n",
    "        1/255, (416, 416)) #mean removed with val of 127.5\n",
    "    # pass the blob through the network and obtain the detections and\n",
    "    # predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\t\n",
    "    # print(detections)\n",
    "    # print(detections.shape)\n",
    "    #loop over the detections\n",
    "    for i in np.arange(0, detections.shape[1]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > .20:\n",
    "            # extract the index of the class label from the\n",
    "            # `detections`, then compute the (x, y)-coordinates of\n",
    "            # the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "                confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "    #show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.waitKey(10)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    # update the FPS counter\n",
    "    #fps.update()\n",
    "\n",
    "    # stop the timer and display FPS information\n",
    "#fps.stop()\n",
    "#print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "#print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(10)\n",
    "#vs.stop()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] loading model...\n[INFO] starting video stream...\n0.48564035\n0.28260347\n0.47334367\n0.27254945\n0.48564035\n0.28260347\n0.48564035\n0.28260347\n0.3921581\n0.3879577\n0.3882846\n0.3921581\nDone processing !!!\nOutput file is stored as  TIL_vidtest_yolo_out_py_notsofast.avi\n"
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#from imutils.video import VideoStream, FileVideoStream\n",
    "#from imutils.video import FPS\n",
    "import numpy as np\n",
    "\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2 \n",
    "\n",
    "# Initialize the parameters\n",
    "confThreshold = 0  #Confidence threshold\n",
    "nmsThreshold = 0.4   #Non-maximum suppression threshold\n",
    "inpWidth = 416       #Width of network's input image\n",
    "inpHeight = 416      #Height of network's input image\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "path = \"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\\"\n",
    "classesFile = path+\"modanet.names\"\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "outputFile = 'TIL_vidtest_yolo_out_py_notsofast.avi'\n",
    "\n",
    "\n",
    "# Get the video writer initialized to save the output video\n",
    "\n",
    "\n",
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = path+\"yolov3-modanet.cfg\"\n",
    "modelWeights = path+\"yolov3-modanet_last.weights\"\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "#net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "#cap = VideoStream(src=0).start()\n",
    "cap = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "\n",
    "# Draw the predicted bounding box\n",
    "def drawPred(classId, conf, left, top, right, bottom):\n",
    "    # Draw a bounding box.\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255))\n",
    "\n",
    "    label = '%.2f' % conf\n",
    "\n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s:%s' % (classes[classId], label)\n",
    "\n",
    "        #Display the label at the top of the bounding box\n",
    "        labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        top = max(top, labelSize[1])\n",
    "        cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255))\n",
    "\n",
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "# Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "\n",
    "#object detected, distance from center -ve if left +ve if right,  return class id as 0 until it height is enough / confidence high enough\n",
    "\n",
    "BBoxes = []\n",
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            \n",
    "            if confidence > confThreshold:\n",
    "                print(confidence)\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                width = int(detection[2] * frameWidth)\n",
    "                height = int(detection[3] * frameHeight)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "                BBoxes.append((center_x,center_y,height))\n",
    "                \n",
    "                \n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        drawPred(classIds[i], confidences[i], left, top, left + width, top + height)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "vid_writer = cv2.VideoWriter(outputFile, cv2.VideoWriter_fourcc('M','J','P','G'), 30, (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "\n",
    "while not cap.isOpened():\n",
    "    cap = cv2.VideoCapture('TIL_vidtest.mp4')\n",
    "    \n",
    "    print(\"loading video...\")\n",
    "\n",
    "    # esc to close stream\n",
    "    if cv2.waitKey(16) == 27:\n",
    "        break\n",
    "counter = 0\n",
    "while cv2.waitKey(1) < 0:\n",
    "    if counter % 20 == 0:\n",
    "    \n",
    "    # get frame from the video\n",
    "        hasFrame, frame = cap.read()\n",
    "    \n",
    "        # Stop the program if reached end of video\n",
    "        if not hasFrame:\n",
    "            print(\"Done processing !!!\")\n",
    "            print(\"Output file is stored as \", outputFile)\n",
    "            cv2.waitKey(3000)\n",
    "            break\n",
    "\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n",
    "        # print(\"blob created\")\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(getOutputsNames(net))\n",
    "        # print(\"reaching post process\")\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        postprocess(frame, outs)\n",
    "\n",
    "        # Put efficiency information. The function getPerfProfile returns the\n",
    "        # overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
    "        t, _ = net.getPerfProfile()\n",
    "        label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "        cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "        # Write the frame with the detection boxes\n",
    "    \n",
    "        vid_writer.write(frame.astype(np.uint8))\n",
    "        counter += 1\n",
    "    else:\n",
    "        hasFrame = cap.grab()\n",
    "        counter += 1\n",
    "\n",
    "#     #show the output frame\n",
    "#     cv2.imshow(\"Frame\", frame)\n",
    "#     cv2.waitKey(10)\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "#     # if the `q` key was pressed, break from the loop\n",
    "#     if key == ord(\"q\"):\n",
    "#         break\n",
    "#     # update the FPS counter\n",
    "#     #fps.update()\n",
    "\n",
    "#     # stop the timer and display FPS information\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #fps.stop()\n",
    "# #print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "# #print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "# # do a bit of cleanup\n",
    "# cv2.destroyAllWindows()\n",
    "# cv2.waitKey(10)\n",
    "# #cap.stop()\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('3.avi')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    cv.imshow('frame', frame)\n",
    "    if cv.waitKey(10) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Can't receive frame (stream end?). Exiting ...\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture('3.avi')\n",
    "i=0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    #cv.imshow('frame', frame)\n",
    "    cv.imwrite('C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL-2020\\\\CV\\\\Raphury\\\\Uncroppped\\\\VideoFrame_uncropped'+str(i)+'.jpg',frame)\n",
    "    i+=1\n",
    "    #if cv.waitKey(10) == ord('q'):\n",
    "    #    break\n",
    "    \n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4.2.0\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nDone processing !!!\nOutput file is stored \n"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "#path = 'C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL-2020\\\\CV\\\\Raphury\\\\Uncroppped\\\\'\n",
    "print(cv2.__version__)\n",
    "vidcap = cv2.VideoCapture('3.avi')\n",
    "\n",
    "count = 0\n",
    "counter = 0\n",
    "while vidcap.isOpened():\n",
    "         # save frame as JPEG file\n",
    "    if counter % 240 == 0:\n",
    "        \n",
    "        success,image = vidcap.read()\n",
    "        if not success:\n",
    "            print(\"Done processing !!!\")\n",
    "            print(\"Output file is stored \")\n",
    "            cv2.waitKey(3000)\n",
    "            break\n",
    "        #cv2.imwrite(os.path.join(path , \"VideoFrame_uncropped%d.jpg\" % count), image)     # save frame as JPEG file\n",
    "        cv2.imwrite(\"VideoFrame_uncropped%d.jpg\" % count, image)\n",
    "        print ('Read as new frame')\n",
    "  \n",
    "        counter += 1\n",
    "        count += 1\n",
    "    else:\n",
    "        success = vidcap.grab()\n",
    "        # hasFrame = cap.grab()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nRead as new frame\nDone processing !!!\nOutput file is stored \n"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "#path = 'C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL-2020\\\\CV\\\\Raphury\\\\Uncroppped\\\\'\n",
    "print(cv2.__version__)\n",
    "vidcap = cv2.VideoCapture('3.avi')\n",
    "\n",
    "def crop_frame_by_7(frame):\n",
    "    crop_by = 7\n",
    "    width = (1280/crop_by)/2\n",
    "    x_left = int((640) - width) # from center\n",
    "    x_right = int((640) + width) #from center\n",
    "    return frame[0:720,x_left:x_right]\n",
    "\n",
    "count = 0\n",
    "counter = 0\n",
    "while vidcap.isOpened():\n",
    "         # save frame as JPEG file\n",
    "    if counter % 1 == 0:\n",
    "        \n",
    "        success,image = vidcap.read()\n",
    "        if not success:\n",
    "            print(\"Done processing !!!\")\n",
    "            print(\"Output file is stored \")\n",
    "            cv2.waitKey(3000)\n",
    "            break\n",
    "        image_crop = crop_frame_by_7(image)\n",
    "         # save frame as JPEG file\n",
    "        cv2.imwrite(\"VideoFrame_uncropped%d.jpg\" % count, image_crop)\n",
    "        print ('Read as new frame')\n",
    "  \n",
    "        counter += 1\n",
    "        count += 1\n",
    "    else:\n",
    "        success = vidcap.grab()\n",
    "        # hasFrame = cap.grab()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4.2.0\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n[[1.]]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4f65e2e35128>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1280\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1280\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1280\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1280\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m          \u001b[1;31m# save frame as JPEG file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#cv.imshow('frame', frame)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4f65e2e35128>\u001b[0m in \u001b[0;36mpredict_binary\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m     87\u001b[0m           method.__name__))\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m             \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    616\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.virtualenvs\\TIL-2020-oa6RrwB_\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#path = 'C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL-2020\\\\CV\\\\Raphury\\\\Uncroppped\\\\'\n",
    "print(cv2.__version__)\n",
    "vidcap = cv2.VideoCapture('3.avi')\n",
    "\n",
    "model = tf.keras.models.load_model(\"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\Xception_binary_snapshot_01.h5\")\n",
    "\n",
    "def predict_binary(frame):\n",
    "    res = model.predict(frame)   \n",
    "    return res\n",
    "\n",
    "count = 0\n",
    "counter = 0\n",
    "while vidcap.isOpened():\n",
    "         # save frame as JPEG file\n",
    "    if counter % 30 == 0:\n",
    "        \n",
    "        success,image = vidcap.read()\n",
    "        if not success:\n",
    "            print(\"Done processing !!!\")\n",
    "            print(\"Output file is stored \")\n",
    "            cv2.waitKey(3000)\n",
    "            break\n",
    "        image = cv2.resize(image, (1280, 1280))\n",
    "        image = np.array(image).reshape((1, 1280, 1280, 3))\n",
    "        text = predict_binary(image)\n",
    "         # save frame as JPEG file\n",
    "        #cv.imshow('frame', frame)\n",
    "        print (text)\n",
    "  \n",
    "        counter += 1\n",
    "        count += 1\n",
    "    else:\n",
    "        success = vidcap.grab()\n",
    "        # hasFrame = cap.grab()\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1.]]\n"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "image = cv2.imread(\"VideoFrame_uncropped2.jpg\")\n",
    "image = cv2.resize(image, (1280, 1280))\n",
    "image = np.array(image).reshape((1, 1280, 1280, 3))\n",
    "\n",
    "model = tf.keras.models.load_model(\"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\Xception_binary_snapshot_01.h5\")\n",
    "res = model.predict(image)\n",
    "def predict_binary(frame):\n",
    "    res = model.predict(frame)   \n",
    "    return res\n",
    "print(predict_binary(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9f488af745a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxception\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_image_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhorizontal_flip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbrightness_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_shift_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#image = cv2.imread(\"VideoFrame_uncropped2.jpg\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_image_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"VideoFrame_uncropped2.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "train_image_generator = ImageDataGenerator(horizontal_flip=True,brightness_range=(0.75,1.25), channel_shift_range=100, preprocessing_function=preprocess_input) \n",
    "#image = cv2.imread(\"VideoFrame_uncropped2.jpg\")\n",
    "image = train_image_generator.flow_from_directory(\"VideoFrame_uncropped2.jpg\")\n",
    "\n",
    "model = tf.keras.models.load_model(\"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\Xception_binary_snapshot_01.h5\")\n",
    "def predict_binary(frame):\n",
    "    res = model.predict(frame)   \n",
    "    return res\n",
    "print(predict_binary(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#path = 'C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL-2020\\\\CV\\\\Raphury\\\\Uncroppped\\\\'\n",
    "print(cv2.__version__)\n",
    "vidcap = cv2.VideoCapture('3.avi')\n",
    "\n",
    "model = tf.keras.models.load_model(\"C:\\\\Users\\\\Raphael\\\\Documents\\\\TIL_yolo_model\\\\Xception_binary_snapshot_01.h5\")\n",
    "\n",
    "def predict_binary(frame):\n",
    "    res = model.predict(frame)   \n",
    "    return res\n",
    "\n",
    "count = 0\n",
    "counter = 0\n",
    "while vidcap.isOpened():\n",
    "         # save frame as JPEG file\n",
    "    if counter % 30 == 0:\n",
    "        \n",
    "        success,image = vidcap.read()\n",
    "        if not success:\n",
    "            print(\"Done processing !!!\")\n",
    "            print(\"Output file is stored \")\n",
    "            cv2.waitKey(3000)\n",
    "            break\n",
    "        image = cv2.resize(image, (1280, 1280))\n",
    "        image = np.array(image).reshape((1, 1280, 1280, 3))\n",
    "        res = predict_binary(frame)\n",
    "\n",
    "\n",
    "        font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        bottomLeftCornerOfText = (10,500)\n",
    "        fontScale              = 1\n",
    "        fontColor              = (255,255,255)\n",
    "        lineType               = 2\n",
    "\n",
    "        cv2.putText(frame,res, \n",
    "            bottomLeftCornerOfText, \n",
    "            font, \n",
    "            fontScale,\n",
    "            fontColor,\n",
    "            lineType)\n",
    "\n",
    "        \n",
    "        cv2.imshow(frame)\n",
    "  \n",
    "        counter += 1\n",
    "        count += 1\n",
    "    else:\n",
    "        success = vidcap.grab()\n",
    "        # hasFrame = cap.grab()\n",
    "        counter += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bittil2020pipenv2f21c983fd1542a19b317973bf347e7d",
   "display_name": "Python 3.6.8 64-bit ('TIL-2020': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}